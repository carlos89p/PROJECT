{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosillanaldariz/miniconda3/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripción agregada a 'transcriptions.csv':\n",
      " Hello, my name is Romain and my favorite day of the week is Monday because from 8 to 9.50 we have PE, then we have a 15 minute break, then we have PE again from 10.00 5.00 to 11.00. It is so fun. After that from 11.00 to 11.55 we have music, then we have lunch from 11.55 to 1.10. After lunch from 1.10 to 2.5 we have French and to finish off the day we have Spanish. I also like Wednesdays because from 1.10 to 2.5 we have art that is one of my very favorite classes. I don't really like Tuesdays because there isn't any of my favorite classes and we have to start the day off with math. Thank you for listening. Bye!\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Cargar modelo de Whisper (elige \"tiny\", \"base\", \"small\", \"medium\" o \"large\")\n",
    "model = whisper.load_model(\"small\")  # Puedes cambiar el tamaño del modelo\n",
    "\n",
    "# Ruta del archivo de audio (modifica con tu archivo)\n",
    "audio_path = \"audio_test.mp3\"  # Puede ser .mp3, .wav, .m4a, .ogg, etc.\n",
    "\n",
    "# Transcribir el audio\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "# Extraer el texto transcrito\n",
    "transcription = result[\"text\"]\n",
    "\n",
    "# Definir nombre del archivo CSV\n",
    "csv_file = \"transcriptions.csv\"\n",
    "\n",
    "# Verificar si el archivo existe para escribir encabezados solo si es la primera vez\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# Crear DataFrame con la nueva transcripción\n",
    "df = pd.DataFrame({\"Texto\": [transcription]})\n",
    "\n",
    "# Guardar en el CSV sin sobrescribir datos anteriores (modo 'a' de append)\n",
    "df.to_csv(csv_file, mode='a', header=not file_exists, index=False, encoding='utf-8')\n",
    "\n",
    "# Imprimir la transcripción\n",
    "print(\"Transcripción agregada a 'transcriptions.csv':\")\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'my', 'name', 'is', 'Romain', 'and', 'my', 'favorite', 'day', 'of', 'the', 'week', 'is', 'Monday', 'because', 'from', '8', 'to', '9.50', 'we', 'have', 'PE', ',', 'then', 'we', 'have', 'a', '15', 'minute', 'break', ',', 'then', 'we', 'have', 'PE', 'again', 'from', '10.00', '5.00', 'to', '11.00', '.', 'It', 'is', 'so', 'fun', '.', 'After', 'that', 'from', '11.00', 'to', '11.55', 'we', 'have', 'music', ',', 'then', 'we', 'have', 'lunch', 'from', '11.55', 'to', '1.10', '.', 'After', 'lunch', 'from', '1.10', 'to', '2.5', 'we', 'have', 'French', 'and', 'to', 'finish', 'off', 'the', 'day', 'we', 'have', 'Spanish', '.', 'I', 'also', 'like', 'Wednesdays', 'because', 'from', '1.10', 'to', '2.5', 'we', 'have', 'art', 'that', 'is', 'one', 'of', 'my', 'very', 'favorite', 'classes', '.', 'I', 'do', \"n't\", 'really', 'like', 'Tuesdays', 'because', 'there', 'is', \"n't\", 'any', 'of', 'my', 'favorite', 'classes', 'and', 'we', 'have', 'to', 'start', 'the', 'day', 'off', 'with', 'math', '.', 'Thank', 'you', 'for', 'listening', '.', 'Bye', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Load the text from the csv file\n",
    "df = pd.read_csv(\"transcriptions.csv\")\n",
    "\n",
    "# Tokenize the last entry in the 'Texto' column\n",
    "if not df.empty and 'Texto' in df.columns:\n",
    "    text = df['Texto'].iloc[-1]  # Get the last entry\n",
    "    tokens = nltk.word_tokenize(str(text))  # Convert to string in case of NaN\n",
    "    print(tokens)\n",
    "else:\n",
    "    print(\"The CSV file is empty or does not contain the 'Texto' column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.845, 'pos': 0.155, 'compound': 0.964}\n"
     ]
    }
   ],
   "source": [
    "#We realize sentiment analysis from the tokenized text\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Get the sentiment score\n",
    "sentiment_score = sia.polarity_scores(text)\n",
    "print(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  Hello , my name is → LABEL_0 (score: 1.00)\n",
      "-  Romain → LABEL_1 (score: 0.99)\n",
      "-  and my favorite day of the week is Monday because from 8 to 9.50 we have PE , then we have a 15 minute break , then we have PE again from 10.00 5.00 to 11.00 . It is so fun . After that from 11.00 to 11.55 we have music , then we have lunch from 11.55 to 1.10 . After lunch from 1.10 to 2.5 we have → LABEL_0 (score: 0.99)\n",
      "-  French → LABEL_7 (score: 1.00)\n",
      "-  and to finish off the day we have → LABEL_0 (score: 1.00)\n",
      "-  Spanish → LABEL_7 (score: 1.00)\n",
      "-  . I also like Wednesdays because from 1.10 to 2.5 we have art that is one of my very favorite classes . I do n't really like Tuesdays because there is n't any of my favorite classes and we have to start the day off with math . Thank you for listening . Bye ! → LABEL_0 (score: 1.00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"  # Evita que use TensorFlow\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Convertimos la lista de tokens en una cadena\n",
    "texto = \" \".join(tokens)\n",
    "\n",
    "# Cargamos el pipeline de NER usando PyTorch y código remoto\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"AventIQ-AI/roberta-named-entity-recognition\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    trust_remote_code=True,\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# Aplicamos el modelo al texto\n",
    "entidades = ner_pipeline(texto)\n",
    "\n",
    "# Mostramos las entidades detectadas\n",
    "for entidad in entidades:\n",
    "    print(f\"- {entidad['word']} → {entidad['entity_group']} (score: {entidad['score']:.2f})\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
