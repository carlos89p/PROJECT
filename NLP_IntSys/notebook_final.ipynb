{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whisper implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carlosillanaldariz/miniconda3/lib/python3.11/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcripción agregada a 'transcriptions.csv':\n",
      " Hello, my name is Romain and my favorite day of the week is Monday because from 8 to 9.50 we have PE, then we have a 15 minute break, then we have PE again from 10.00 5.00 to 11.00. It is so fun. After that from 11.00 to 11.55 we have music, then we have lunch from 11.55 to 1.10. After lunch from 1.10 to 2.5 we have French and to finish off the day we have Spanish. I also like Wednesdays because from 1.10 to 2.5 we have art that is one of my very favorite classes. I don't really like Tuesdays because there isn't any of my favorite classes and we have to start the day off with math. Thank you for listening. Bye!\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Cargar modelo de Whisper (elige \"tiny\", \"base\", \"small\", \"medium\" o \"large\")\n",
    "model = whisper.load_model(\"small\")  # Puedes cambiar el tamaño del modelo\n",
    "\n",
    "# Ruta del archivo de audio (modifica con tu archivo)\n",
    "audio_path = \"audio_test.mp3\"  # Puede ser .mp3, .wav, .m4a, .ogg, etc.\n",
    "\n",
    "# Transcribir el audio\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "# Extraer el texto transcrito\n",
    "transcription = result[\"text\"]\n",
    "\n",
    "# Definir nombre del archivo CSV\n",
    "csv_file = \"transcriptions.csv\"\n",
    "\n",
    "# Verificar si el archivo existe para escribir encabezados solo si es la primera vez\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "# Crear DataFrame con la nueva transcripción\n",
    "df = pd.DataFrame({\"Texto\": [transcription]})\n",
    "\n",
    "# Guardar en el CSV sin sobrescribir datos anteriores (modo 'a' de append)\n",
    "df.to_csv(csv_file, mode='a', header=not file_exists, index=False, encoding='utf-8')\n",
    "\n",
    "# Imprimir la transcripción\n",
    "print(\"Transcripción agregada a 'transcriptions.csv':\")\n",
    "print(transcription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'my', 'name', 'is', 'Romain', 'and', 'my', 'favorite', 'day', 'of', 'the', 'week', 'is', 'Monday', 'because', 'from', '8', 'to', '9.50', 'we', 'have', 'PE', ',', 'then', 'we', 'have', 'a', '15', 'minute', 'break', ',', 'then', 'we', 'have', 'PE', 'again', 'from', '10.00', '5.00', 'to', '11.00', '.', 'It', 'is', 'so', 'fun', '.', 'After', 'that', 'from', '11.00', 'to', '11.55', 'we', 'have', 'music', ',', 'then', 'we', 'have', 'lunch', 'from', '11.55', 'to', '1.10', '.', 'After', 'lunch', 'from', '1.10', 'to', '2.5', 'we', 'have', 'French', 'and', 'to', 'finish', 'off', 'the', 'day', 'we', 'have', 'Spanish', '.', 'I', 'also', 'like', 'Wednesdays', 'because', 'from', '1.10', 'to', '2.5', 'we', 'have', 'art', 'that', 'is', 'one', 'of', 'my', 'very', 'favorite', 'classes', '.', 'I', 'do', \"n't\", 'really', 'like', 'Tuesdays', 'because', 'there', 'is', \"n't\", 'any', 'of', 'my', 'favorite', 'classes', 'and', 'we', 'have', 'to', 'start', 'the', 'day', 'off', 'with', 'math', '.', 'Thank', 'you', 'for', 'listening', '.', 'Bye', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Load the text from the csv file\n",
    "df = pd.read_csv(\"transcriptions.csv\")\n",
    "\n",
    "# Tokenize the last entry in the 'Texto' column\n",
    "if not df.empty and 'Texto' in df.columns:\n",
    "    text = df['Texto'].iloc[-1]  # Get the last entry\n",
    "    tokens = nltk.word_tokenize(str(text))  # Convert to string in case of NaN\n",
    "    print(tokens)\n",
    "else:\n",
    "    print(\"The CSV file is empty or does not contain the 'Texto' column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.845, 'pos': 0.155, 'compound': 0.964}\n"
     ]
    }
   ],
   "source": [
    "#We realize sentiment analysis from the tokenized text\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Get the sentiment score\n",
    "sentiment_score = sia.polarity_scores(text)\n",
    "print(sentiment_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  Hello , my name is → LABEL_0 (score: 1.00)\n",
      "-  Romain → LABEL_1 (score: 0.99)\n",
      "-  and my favorite day of the week is Monday because from 8 to 9.50 we have PE , then we have a 15 minute break , then we have PE again from 10.00 5.00 to 11.00 . It is so fun . After that from 11.00 to 11.55 we have music , then we have lunch from 11.55 to 1.10 . After lunch from 1.10 to 2.5 we have → LABEL_0 (score: 0.99)\n",
      "-  French → LABEL_7 (score: 1.00)\n",
      "-  and to finish off the day we have → LABEL_0 (score: 1.00)\n",
      "-  Spanish → LABEL_7 (score: 1.00)\n",
      "-  . I also like Wednesdays because from 1.10 to 2.5 we have art that is one of my very favorite classes . I do n't really like Tuesdays because there is n't any of my favorite classes and we have to start the day off with math . Thank you for listening . Bye ! → LABEL_0 (score: 1.00)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"  # Evita que use TensorFlow\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Convertimos la lista de tokens en una cadena\n",
    "texto = \" \".join(tokens)\n",
    "\n",
    "# Cargamos el pipeline de NER usando PyTorch y código remoto\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"AventIQ-AI/roberta-named-entity-recognition\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    trust_remote_code=True,\n",
    "    framework=\"pt\"\n",
    ")\n",
    "\n",
    "# Aplicamos el modelo al texto\n",
    "entidades = ner_pipeline(texto)\n",
    "\n",
    "# Mostramos las entidades detectadas\n",
    "for entidad in entidades:\n",
    "    print(f\"- {entidad['word']} → {entidad['entity_group']} (score: {entidad['score']:.2f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Cargar el modelo y el tokenizador\n",
    "model_name = \"AventIQ-AI/roberta-named-entity-recognition\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_name)\n",
    "model = RobertaForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Etiquetas del modelo\n",
    "label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\n",
    "\n",
    "def extract_cities(text):\n",
    "    # Tokenizar el texto\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    # Obtener las predicciones del modelo\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "    \n",
    "    predictions = torch.argmax(outputs.logits, dim=2)\n",
    "    \n",
    "    # Convertir las predicciones en etiquetas y tokens\n",
    "    tokens_list = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])\n",
    "    predicted_labels = [label_list[pred] for pred in predictions[0].numpy()]\n",
    "    \n",
    "    # Extraer las ciudades reconocidas\n",
    "    cities = []\n",
    "    current_city = []\n",
    "    for token, label in zip(tokens_list, predicted_labels):\n",
    "        if label == \"B-LOC\":\n",
    "            if current_city:\n",
    "                cities.append(\"\".join(current_city).replace(\"Ġ\", \" \").strip())\n",
    "                current_city = []\n",
    "            current_city.append(token)\n",
    "        elif label == \"I-LOC\" and current_city:\n",
    "            current_city.append(token)\n",
    "        else:\n",
    "            if current_city:\n",
    "                cities.append(\"\".join(current_city).replace(\"Ġ\", \" \").strip())\n",
    "                current_city = []\n",
    "    if current_city:\n",
    "        cities.append(\"\".join(current_city).replace(\"Ġ\", \" \").strip())\n",
    "    \n",
    "    return cities\n",
    "\n",
    "# Texto de ejemplo\n",
    "texto = \"Me gustaría viajar desde Madrid hasta Barcelona pasando por Zaragoza.\"\n",
    "\n",
    "# Extraer ciudades del texto\n",
    "ciudades_mencionadas = extract_cities(texto)\n",
    "\n",
    "# Asignar las ciudades a las variables 'inicio' y 'destino'\n",
    "inicio = destino = None\n",
    "if len(ciudades_mencionadas) == 1:\n",
    "    destino = ciudades_mencionadas[0]\n",
    "elif len(ciudades_mencionadas) >= 2:\n",
    "    inicio = ciudades_mencionadas[0]\n",
    "    destino = ciudades_mencionadas[1]\n",
    "\n",
    "print(\"Inicio:\", inicio)\n",
    "print(\"Destino:\", destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grafo = 'nuevo_graf.txt'\n",
    "\n",
    "inicio = 'Ribadeo'\n",
    "destino = 'Verin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar datos del archivo\n",
    "def cargar_mapa(path):\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    nodos = {}\n",
    "    aristas = []\n",
    "    leyendo_nodos = False\n",
    "    leyendo_aristas = False\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            if \"Nodos\" in line:\n",
    "                leyendo_nodos = True\n",
    "                leyendo_aristas = False\n",
    "            elif \"Aristas\" in line:\n",
    "                leyendo_aristas = True\n",
    "                leyendo_nodos = False\n",
    "            continue\n",
    "\n",
    "        if leyendo_nodos:\n",
    "            ciudad, lat, lon = line.split(',')\n",
    "            nodos[ciudad] = (float(lat), float(lon))\n",
    "        elif leyendo_aristas:\n",
    "            origen, destino, distancia = line.split(',')\n",
    "            aristas.append((origen, destino, float(distancia)))  # <-- CAMBIO AQUÍ\n",
    "\n",
    "    return nodos, aristas\n",
    "\n",
    "# Crear y visualizar el grafo\n",
    "def visualizar_mapa(path):\n",
    "    nodos, aristas = cargar_mapa(path)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Añadir nodos con posición geográfica\n",
    "    for ciudad, (lat, lon) in nodos.items():\n",
    "        G.add_node(ciudad, pos=(lon, lat))  # Nota: X = longitud, Y = latitud\n",
    "\n",
    "    # Añadir aristas con distancia\n",
    "    for origen, destino, distancia in aristas:\n",
    "        G.add_edge(origen, destino, weight=distancia)\n",
    "\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=300, node_color='skyblue')\n",
    "    nx.draw_networkx_edges(G, pos, width=1.5, alpha=0.7)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=9)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels, font_size=7)\n",
    "\n",
    "    plt.title(\"Galicia Map - Cities, Distances and Connections\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ruta al archivo de texto\n",
    "visualizar_mapa(grafo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "# Calcular distancia Haversine entre coordenadas\n",
    "def haversine(coord1, coord2):\n",
    "    R = 6371\n",
    "    lat1, lon1 = map(math.radians, coord1)\n",
    "    lat2, lon2 = map(math.radians, coord2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "# Leer nodos y aristas del archivo\n",
    "def load_graph(file_path):\n",
    "    nodes = {}\n",
    "    graph = {}\n",
    "    with open(file_path, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines) and not lines[i].startswith(\"# Aristas\"):\n",
    "        if not lines[i].startswith(\"#\") and lines[i].strip():\n",
    "            city, lat, lon = lines[i].strip().split(\",\")\n",
    "            nodes[city] = (float(lat), float(lon))\n",
    "            graph[city] = {}\n",
    "        i += 1\n",
    "\n",
    "    while i < len(lines):\n",
    "        if not lines[i].startswith(\"#\") and lines[i].strip():\n",
    "            parts = lines[i].strip().split(\",\")\n",
    "            if len(parts) == 3:\n",
    "                city1, city2, dist = parts\n",
    "                dist = float(dist)\n",
    "                graph[city1][city2] = dist\n",
    "                graph[city2][city1] = dist\n",
    "        i += 1\n",
    "\n",
    "    return nodes, graph\n",
    "\n",
    "# Algoritmo Greedy Best-First Search\n",
    "def greedy_best_first(graph, nodes, start, goal):\n",
    "    visited = set()\n",
    "    queue = []\n",
    "    heapq.heappush(queue, (haversine(nodes[start], nodes[goal]), 0, start, [start]))\n",
    "\n",
    "    while queue:\n",
    "        heuristic_dist, path_dist, current, path = heapq.heappop(queue)\n",
    "\n",
    "        if current == goal:\n",
    "            return path, path_dist\n",
    "\n",
    "        if current in visited:\n",
    "            continue\n",
    "        visited.add(current)\n",
    "\n",
    "        for neighbor in graph[current]:\n",
    "            if neighbor not in visited:\n",
    "                new_path = path + [neighbor]\n",
    "                new_cost = path_dist + graph[current][neighbor]\n",
    "                heuristic = haversine(nodes[neighbor], nodes[goal])\n",
    "                heapq.heappush(queue, (heuristic, new_cost, neighbor, new_path))\n",
    "\n",
    "    return [], float('inf')  # Si no hay camino\n",
    "\n",
    "# MAIN\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = grafo\n",
    "    nodes, graph = load_graph(file_path)\n",
    "\n",
    "    start_city = inicio\n",
    "    end_city = destino\n",
    "\n",
    "    if start_city not in nodes or end_city not in nodes:\n",
    "        print(\"Ciudad de inicio o fin no válida.\")\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        path, total_dist = greedy_best_first(graph, nodes, start_city, end_city)\n",
    "        end_time = time.time()\n",
    "\n",
    "        if path:\n",
    "            print(\"Best route found:\", \" -> \".join(path))\n",
    "            print(f\"Total distance: {total_dist:.2f} km\")\n",
    "        else:\n",
    "            print(\"No se encontró un camino.\")\n",
    "        print(\"Execution time:\", round(end_time - start_time, 4), \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualizar_ruta(path_archivo, ruta=None):\n",
    "    nodos, aristas = cargar_mapa(path_archivo)\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for ciudad, (lat, lon) in nodos.items():\n",
    "        G.add_node(ciudad, pos=(lon, lat))\n",
    "\n",
    "    for origen, destino, distancia in aristas:\n",
    "        G.add_edge(origen, destino, weight=distancia)\n",
    "\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "\n",
    "    # Colores de los nodos\n",
    "    node_colors = []\n",
    "    for nodo in G.nodes():\n",
    "        if ruta and nodo in ruta:\n",
    "            node_colors.append('red')\n",
    "        else:\n",
    "            node_colors.append('skyblue')\n",
    "\n",
    "    # Aristas que están en la ruta (pares consecutivos)\n",
    "    ruta_edges = []\n",
    "    if ruta:\n",
    "        ruta_edges = [(ruta[i], ruta[i+1]) for i in range(len(ruta)-1)]\n",
    "\n",
    "    # Dibujar nodos y etiquetas\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=300, node_color=node_colors)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=9)\n",
    "\n",
    "    # Dibujar aristas\n",
    "    edge_colors = []\n",
    "    for edge in G.edges():\n",
    "        if edge in ruta_edges or (edge[1], edge[0]) in ruta_edges:\n",
    "            edge_colors.append('red')\n",
    "        else:\n",
    "            edge_colors.append('gray')\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, width=1.5, alpha=0.7, edge_color=edge_colors)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels, font_size=7)\n",
    "\n",
    "    plt.title(\"Ideal route in red\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Ejemplo de uso\n",
    "visualizar_ruta(grafo, ruta=path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
